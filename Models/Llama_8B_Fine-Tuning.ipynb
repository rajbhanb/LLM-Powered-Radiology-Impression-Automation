{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).\n",
        "\n",
        "[NEW] Llama-3.1 8b, 70b & 405b are trained on a crazy 15 trillion tokens with 128K long context lengths!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth \"xformers==0.0.28.post2\"\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOCp9ilKofzF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U bitsandbytes transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
        "* [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
        "* [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Llama 8B Model"
      ],
      "metadata": {
        "id": "Mi-hcaDlb0ox"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "f880217a-5b0a-4feb-db92-9391d255a4c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.3.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.0. CUDA Toolkit = 12.4.\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "CPU times: user 20.3 s, sys: 2.58 s, total: 22.9 s\n",
            "Wall time: 20.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 500 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Stw-9tA61CU",
        "outputId": "c85e7813-4ced-4148-83d8-8ddebdfdbfb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "2a86d734-9b1e-404a-d7a9-9dd68aca0426"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.11.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkiLh5SJv6I3",
        "outputId": "6a28151b-23dd-492f-928d-3759da7c0163"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOH1TE_iK2DP",
        "outputId": "3dde682f-320f-42b1-a0bc-2d0087894285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Umm6YYK5KPJ3",
        "outputId": "4a1d9cc4-2eec-4a6c-b320-0a5878b7bb95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 27484 entries, 0 to 33900\n",
            "Data columns (total 11 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   Unnamed: 0            27484 non-null  int64  \n",
            " 1   clinical_information  27484 non-null  object \n",
            " 2   technique             27484 non-null  object \n",
            " 3   findings              27484 non-null  object \n",
            " 4   comparison            23491 non-null  object \n",
            " 5   impression            27484 non-null  object \n",
            " 6   report_id_x           27484 non-null  object \n",
            " 7   join                  27484 non-null  int64  \n",
            " 8   report_id_y           27484 non-null  float64\n",
            " 9   modality              27484 non-null  object \n",
            " 10  instruction           27484 non-null  object \n",
            "dtypes: float64(1), int64(2), object(8)\n",
            "memory usage: 2.5+ MB\n",
            "CPU times: user 708 ms, sys: 79.1 ms, total: 787 ms\n",
            "Wall time: 818 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the dataset\n",
        "MRIdf = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Capstone2/Data/MRI_full.csv')\n",
        "\n",
        "# Remove rows with missing values in the relevant columns\n",
        "MRIdf = MRIdf.dropna(subset=['clinical_information', 'findings', 'impression'])\n",
        "MRIdf.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goal is to fine-tune model to generate impression using clinical information and findings"
      ],
      "metadata": {
        "id": "lQwGXxxjcCwa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726,
          "referenced_widgets": [
            "b8f403f704534f1a8b71de001f2ac038",
            "c9422bd2ad8f47e987ad247f34a3686d",
            "58a2e0cc3a8944dfaee70e8bf662c17e",
            "e4968b65b5904f5ba95d662bb8ca171a",
            "9314d10eafd54b7db4ed1d1026b17a6d",
            "cc3d41938fed42ed98ef94f90220fa5b",
            "9c87a2f2bfd243aba656095f4358351f",
            "2df001d63e544a1e9a2b39471c2b64c2",
            "20e585fb1a97455f99ce317998c0fa51",
            "05646d6858934bf09f6415ed80c5f2b9",
            "3062dbbfa2a04c0cb63acdde2646b799"
          ]
        },
        "id": "phJxNC5ZU6VL",
        "outputId": "1d891165-472f-48d2-bdf4-9fad68f98a72"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8f403f704534f1a8b71de001f2ac038",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/27484 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 550 ms, sys: 264 ms, total: 814 ms\n",
            "Wall time: 806 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'findings': ['There is a heterogeneous left supratentorial and infratentorial lesion with associated cystic compartments, some of which contain fluid-fluid levels suggestive of hemorrhage. There is also surrounding vasogenic edema in the left cerebellar hemisphere. There is marked effacement of the fourth ventricle and dilatation of the third and fourth ventricles, which have increased in size slightly. There is also marked mass effect upon the brainstem. There is also a similar contiguous heterogeneous lesion within the left temporal and occipital lobes with surrounding vasogenic edema. There is no herniation. There are skin-based fiducial markers.',\n",
              "  'No evidence of restricted diffusion is seen. Patchy periventricular T2 hyperintensity is evident, a nonspecific finding which likely represents chronic small vessel ischemic disease. No intracranial hemorrhage or abnormal extra-axial fluid collections are seen. There is no evidence of parenchymal edema or mass effect. The ventricular system is normal in size and morphology. No pathologic parenchymal or extra-axial enhancement is seen.'],\n",
              " 'clinical_information': ['Preoperative planning for brain tumor. History of a left temporal AVM treated with radiation.',\n",
              "  'Female, 61 years old, founded nursing home acutely slumped over bleeding technician, history of stroke with residual weakness of legs, wheelchair-bound.'],\n",
              " 'impression': ['Presurgical planning MRI shows a complex mass in the left cerebellar hemisphere with associated obstructive hydrocephalus and a similar complex mass in the left temporo-occipital region. These lesions may represent radiation necrosis and perhaps vascular malformations with associated hemorrhage.',\n",
              "  '1. No evidence of acute ischemia or other definite acute intracranial abnormality.2. Chronic small vessel ischemic disease.'],\n",
              " '__index_level_0__': [0, 2],\n",
              " 'text': ['\\n\\n### Input:\\nPreoperative planning for brain tumor. History of a left temporal AVM treated with radiation.\\n\\n### Input2:\\nThere is a heterogeneous left supratentorial and infratentorial lesion with associated cystic compartments, some of which contain fluid-fluid levels suggestive of hemorrhage. There is also surrounding vasogenic edema in the left cerebellar hemisphere. There is marked effacement of the fourth ventricle and dilatation of the third and fourth ventricles, which have increased in size slightly. There is also marked mass effect upon the brainstem. There is also a similar contiguous heterogeneous lesion within the left temporal and occipital lobes with surrounding vasogenic edema. There is no herniation. There are skin-based fiducial markers.\\n\\n### Response:\\nPresurgical planning MRI shows a complex mass in the left cerebellar hemisphere with associated obstructive hydrocephalus and a similar complex mass in the left temporo-occipital region. These lesions may represent radiation necrosis and perhaps vascular malformations with associated hemorrhage.<|end_of_text|>',\n",
              "  '\\n\\n### Input:\\nFemale, 61 years old, founded nursing home acutely slumped over bleeding technician, history of stroke with residual weakness of legs, wheelchair-bound.\\n\\n### Input2:\\nNo evidence of restricted diffusion is seen. Patchy periventricular T2 hyperintensity is evident, a nonspecific finding which likely represents chronic small vessel ischemic disease. No intracranial hemorrhage or abnormal extra-axial fluid collections are seen. There is no evidence of parenchymal edema or mass effect. The ventricular system is normal in size and morphology. No pathologic parenchymal or extra-axial enhancement is seen.\\n\\n### Response:\\n1. No evidence of acute ischemia or other definite acute intracranial abnormality.2. Chronic small vessel ischemic disease.<|end_of_text|>']}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "#HAD SAME RESULT AS FINDINGS AND CLINICAL INFO USED AS COMBINED INPUTS\n",
        "\n",
        "\n",
        "# Load only the required columns and drop any rows with missing values in those columns\n",
        "MRIdf_subset = MRIdf[['findings', 'clinical_information', 'impression',]].dropna().drop_duplicates()\n",
        "\n",
        "# Ensure that each column is of type string\n",
        "MRIdf_subset['findings'] = MRIdf_subset['findings'].astype(str)\n",
        "MRIdf_subset['clinical_information'] = MRIdf_subset['clinical_information'].astype(str)\n",
        "MRIdf_subset['impression'] = MRIdf_subset['impression'].astype(str)\n",
        "\n",
        "# Convert the subset DataFrame to a Hugging Face Dataset\n",
        "MRIdf_dataset = Dataset.from_pandas(MRIdf_subset)\n",
        "\n",
        "# Define the Alpaca-style prompt template\n",
        "alpaca_prompt = \"\"\"\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Input2:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# EOS token for end-of-sequence in generated text\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Formatting function to structure input-output pairs\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"clinical_information\"]\n",
        "    inputs2 = examples[\"findings\"]\n",
        "    outputs = examples[\"impression\"]\n",
        "    texts = []\n",
        "    for input, input2, output in zip(inputs, inputs2, outputs):\n",
        "        # Use Alpaca prompt template, adding EOS token at the end\n",
        "        text = alpaca_prompt.format(input, input2, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Apply formatting function to dataset with batched processing\n",
        "formatted_data = MRIdf_dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Show the first few formatted examples\n",
        "formatted_data[:2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAmp4ztDjbkA",
        "outputId": "f2455ea8-b73b-4127-b51d-fde52ed145e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 21987\n",
            "Validation samples: 2748\n",
            "Test samples: 2749\n",
            "CPU times: user 314 ms, sys: 295 ms, total: 609 ms\n",
            "Wall time: 604 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert the formatted dataset to a DataFrame for train-test split, and reset index to avoid duplicate index errors\n",
        "formatted_df = formatted_data.to_pandas().reset_index(drop=True)\n",
        "\n",
        "# First split into train and temp (80-20)\n",
        "train_data, temp_data = train_test_split(formatted_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split temp into validation and test (50-50, meaning 10-10 of original data)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert back to Hugging Face Datasets without additional indices\n",
        "train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
        "val_dataset = Dataset.from_pandas(val_data.reset_index(drop=True))\n",
        "test_dataset = Dataset.from_pandas(test_data.reset_index(drop=True))\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "652d4ee791964e928d28cb9c7f4c5684",
            "ea3db4204db945c1b6886e2d95315061",
            "89b006bdca0f40e89ff986ac81d943c5",
            "a647a2c063f34829a8219bf3d73594c7",
            "125a412329ae4e0c8010a664f588b7aa",
            "3b39fd62f15143669bf5783a2e9b7c4e",
            "d1d9552e1dd7438ea1f8c66f669c97dc",
            "1363f9bef4b24db4b21ba5aa43ad55fc",
            "a59ee7c1299c40f1afcde5e3fefa705d",
            "3709d36325f4441d88171c27487ecf34",
            "7e5b4cb550004fd9abf2c5b0ebd72857",
            "a2b78e931f064940a721c1226e7fb47e",
            "40be3012f4da44af820caf46bc154480",
            "8b3ed67717694c5987cbd87bee4b7825",
            "0821fc7f6fec4fbe816c82841121edee",
            "cc8fd36d8f2d41aa9388fc374041e08e",
            "8d8a326bd297498db74c5f5e88c1acbe",
            "3364c56ad01e48478633cdc44370c43d",
            "6286526939004191a918458e3996a853",
            "d8be3584ab8c4d6e8b512e115eff7880",
            "e3d2284a05f7424887c3c11387258697",
            "f56fb0adbaeb4465b9879ae615b8037e"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "98521a74-5f89-4a99-e7fe-84c6e9f7eed2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "652d4ee791964e928d28cb9c7f4c5684",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/21987 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2b78e931f064940a721c1226e7fb47e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/2748 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1.65 s, sys: 1.15 s, total: 2.81 s\n",
            "Wall time: 17.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "     train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,  # Added validation dataset\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 80,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "#The maximum number of training steps to run. If set to -1, training will continue until the specified number of epochs is reached.\n",
        "        max_steps=-1,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "ae94f6a4-de50-434f-f237-3d658b276352"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.564 GB.\n",
            "5.984 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "8cce3e93-0a28-4a7c-d205-a7e4bd9652de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 21,987 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 80 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 320 | Total steps = 68\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [68/68 48:07, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.038900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.019400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.036700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.963400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.969800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.871800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.806400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.728100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.688100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.610300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.626600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.602900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.553800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.563100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.539100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.488100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.512500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.457300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.461300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.435000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.467200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.394300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.340700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.352800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.365600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.385000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.336000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.348900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.304000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.310500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.295100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.265800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.319000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.251600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.271500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.253700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.287800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.259200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.219500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.298700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.273400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.265400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.251100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.228700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.269500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.281700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.244000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.297900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.254300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.225100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.224400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.230600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.212500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.197500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.201600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.237700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.225900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.207500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.237800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.250900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.231700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.215400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.208200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.241500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.196200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.212500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.209600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 27min 59s, sys: 21min 18s, total: 49min 17s\n",
            "Wall time: 49min\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqOV7wdzkYLh",
        "outputId": "b4172506-5c52-4ea9-f6ff-db11ac14b570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 363 ms, sys: 162 ms, total: 525 ms\n",
            "Wall time: 1.17 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsMRILlama8b_1epoc_stepsclinc/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsMRILlama8b_1epoc_stepsclinc/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsMRILlama8b_1epoc_stepsclinc/tokenizer.json')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "# Save the trained model and tokenizer\n",
        "model.save_pretrained('/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsMRILlama8b_1epoc_stepsclinc')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsMRILlama8b_1epoc_stepsclinc')\n",
        "\n",
        "#Save training arguments\n",
        "#torch.save(training_params, '/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsMRILlama8b60steps')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd2YvrZwmjmj",
        "outputId": "c76ebf6f-4841-4768-997c-dc070ab22fd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 17.7 s, sys: 5.96 s, total: 23.7 s\n",
            "Wall time: 1min 42s\n"
          ]
        }
      ],
      "source": [
        "#Reload Model\n",
        "%%time\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "modelsaved = AutoModelForCausalLM.from_pretrained('/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsMRILlama8b_1epoc_stepsclinc')\n",
        "tokenizersaved = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsMRILlama8b_1epoc_stepsclinc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTWUaUmrQyN7"
      },
      "source": [
        "**START**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SNno2S-hMld",
        "outputId": "4b0d2930-82c0-4527-c097-55b3fd5c42b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|>\n",
            "\n",
            "### Input:\n",
            "Hip pain\n",
            "\n",
            "### Input2:\n",
            "ACETABULAR LABRUM: Focal tear along the anterosuperior aspect at approximately 1-2 o'clock without evidence of significant displacement or additional labral findings throughout the remainder. ARTICULAR CARTILAGE AND BONE: Intact and without focal defects. No abnormal discrete signal abnormality. Marrow signal is consistent with red marrow replacement scattered throughout the pelvis and visualized portions of the proximal femur; unremarkable for age. SOFT TISSUES: No significant abnormality noted. ADDITIONAL\n",
            "\n",
            "### Response:\n",
            "1. Focal tear of the anterosuperior labrum. 2. No focal cartilage defects or marrow signal abnormality to suggest underlying osteonecrosis.<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Prepare the inputs for the model\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            \"Hip pain\",\n",
        "            \"ACETABULAR LABRUM: Focal tear along the anterosuperior aspect at approximately 1-2 o'clock without evidence of significant displacement or additional labral findings throughout the remainder. ARTICULAR CARTILAGE AND BONE: Intact and without focal defects. No abnormal discrete signal abnormality. Marrow signal is consistent with red marrow replacement scattered throughout the pelvis and visualized portions of the proximal femur; unremarkable for age. SOFT TISSUES: No significant abnormality noted. ADDITIONAL\",  # input\n",
        "            \"\"  # output - leave this blank for generation!\n",
        "        )\n",
        "    ],\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Generate the outputs\n",
        "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "\n",
        "# Decode the outputs\n",
        "generated_texts = tokenizer.batch_decode(outputs)\n",
        "\n",
        "# Print the generated text\n",
        "for text in generated_texts:\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8YErONFgtfa",
        "outputId": "7fa008ff-b8a6-4acf-bbfb-a452fd5ec3b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|>\n",
            "### Input:\n",
            "Clinical question: Prior MRI with questionable demyelinating disease. Please assess for new lesions. Signs and symptoms: Cognitive decline/depression\n",
            "### Input2:\n",
            "Pre-and post-enhanced MRI:No diffusion weighted abnormalities. Examination redemonstrates stable few subcortical and periventricular foci of flair hyperintensity in bilateral cerebral hemispheres. There is no evidence of any new lesions or any detectable abnormal enhancement.Previously noted lesions within bilateral middle cerebellar peduncles are significantly less conspicuous on current the study and there is no evidence of any new or enhancing lesions in the posterior fossa.Unremarkable cerebral cortex, cortical sulci, ventricular system and the CSF spaces for patient's stated age. No detectable abnormal parenchymal or leptomeningeal enhancement.Unremarkable images through the orbits and including axial fat sat post enhanced series.Unremarkable calvarium, soft tissues of the scalp, paranasal sinuses and mastoid air cells\n",
            "### Response:\n",
            "1.Pre-and post-enhanced MRI redemonstrates stable few subcortical and periventricular foci of flair hyperintensity in bilateral cerebral hemispheres. There is no evidence of any new lesions or any detectable abnormal enhancement.2.Previously noted lesions within bilateral middle cerebellar peduncles are significantly less conspicuous on current the study and there is no evidence of any new or enhancing lesions in the posterior fossa.3.Unremarkable cerebral cortex, cortical sulci\n"
          ]
        }
      ],
      "source": [
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Prepare the inputs for the model\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            \"Clinical question: Prior MRI with questionable demyelinating disease. Please assess for new lesions. Signs and symptoms: Cognitive decline/depression\",  # input\n",
        "            \"Pre-and post-enhanced MRI:No diffusion weighted abnormalities. Examination redemonstrates stable few subcortical and periventricular foci of flair hyperintensity in bilateral cerebral hemispheres. There is no evidence of any new lesions or any detectable abnormal enhancement.Previously noted lesions within bilateral middle cerebellar peduncles are significantly less conspicuous on current the study and there is no evidence of any new or enhancing lesions in the posterior fossa.Unremarkable cerebral cortex, cortical sulci, ventricular system and the CSF spaces for patient's stated age. No detectable abnormal parenchymal or leptomeningeal enhancement.Unremarkable images through the orbits and including axial fat sat post enhanced series.Unremarkable calvarium, soft tissues of the scalp, paranasal sinuses and mastoid air cells\",\n",
        "            \"\"  # output - leave this blank for generation!\n",
        "        )\n",
        "    ],\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Generate the outputs\n",
        "outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
        "\n",
        "# Decode the outputs\n",
        "generated_texts = tokenizer.batch_decode(outputs)\n",
        "\n",
        "# Print the generated text\n",
        "for text in generated_texts:\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwt1BxSNVaZg",
        "outputId": "037ae84b-6e63-4b91-ae19-9e5a216b1211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|>\n",
            "### Input:\n",
            "Pain and swelling\n",
            "### Input2:\n",
            "Osseous structures: There are destructive and erosive changes of the midfoot, tibiotalar joint, and talocalcaneal joint reflective of Charcot arthropathy. There is no definite marrow abnormality to confirm osteomyelitis. Metallic susceptibility artifact is noted in the medial aspect of the calcaneus which may reflect prior surgical intervention. There is a small anterior joint effusion which contains a probable loose body.Soft tissues: There is diffuse subcutaneous edema although no discrete fluid collection is identified. There is increased signal throughout the muscles of the foot, but particularly within the plantar muscles.\n",
            "### Response:\n",
            "1. Findings consistent with Charcot arthropathy of the midfoot, tibiotalar joint, and talocalcaneal joint. No definite osteomyelitis.2. Diffuse subcutaneous edema and increased signal intensity within the muscles of the foot which may reflect soft tissue infection.<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Prepare the inputs for the model\n",
        "inputs = tokenizer(\n",
        "    [\n",
        "        alpaca_prompt.format(\n",
        "            \"Pain and swelling\",\n",
        "            \"Osseous structures: There are destructive and erosive changes of the midfoot, tibiotalar joint, and talocalcaneal joint reflective of Charcot arthropathy. There is no definite marrow abnormality to confirm osteomyelitis. Metallic susceptibility artifact is noted in the medial aspect of the calcaneus which may reflect prior surgical intervention. There is a small anterior joint effusion which contains a probable loose body.Soft tissues: There is diffuse subcutaneous edema although no discrete fluid collection is identified. There is increased signal throughout the muscles of the foot, but particularly within the plantar muscles.\",  # input\n",
        "            \"\"  # output - leave this blank for generation!\n",
        "        )\n",
        "    ],\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Generate the outputs\n",
        "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "\n",
        "# Decode the outputs\n",
        "generated_texts = tokenizer.batch_decode(outputs)\n",
        "\n",
        "# Print the generated text\n",
        "for text in generated_texts:\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate impression using clinical information and findings from test dataset using fine-tuned model"
      ],
      "metadata": {
        "id": "v7qcb_bjdHN_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sztWab9dSWjG",
        "outputId": "a4171cc6-07d8-4cd4-d099-57d082575740"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['findings', 'clinical_information', 'impression', '__index_level_0__', 'text'],\n",
              "    num_rows: 2749\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jISAJmlIb6rI",
        "outputId": "da3e420a-009a-4669-fc29-e69e7827a6a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'findings': 'MENISCI: Marked destruction of the posterior horn of the medial meniscus without extension into the body. Only portions of the medial anchor remains given extensive fragmentation. Anterior horn is otherwise intact. Lateral meniscus is significant for moderate fraying of the inner edge. A small parameniscal cyst is observed adjacent to the anterior horn of the lateral meniscus.ARTICULAR CARTILAGE AND BONE: Mild thinning with focal 1.5 cm defect involving the posterior aspect of the femoral condyle, and overlying the medial meniscal abnormality described above. Mild overlying bone edema. Otherwise moderate changes most pronounced underlying the patella, without additional focal defects. Marrow signal otherwise intact other than scattered degenerative changes with numerous cysts, otherwise observed in the tibial plateau.LIGAMENTS: No significant abnormality noted. EXTENSOR MECHANISM: No significant abnormality noted.ADDITIONAL',\n",
              " 'clinical_information': 'Medial joint pain',\n",
              " 'impression': 'Large knee effusion with associated fragmentation and destruction of the posterior horn of the medial meniscus. See description and the associated overlying adjacent cartilaginous defect.',\n",
              " '__index_level_0__': 28822,\n",
              " 'text': '\\n\\n### Input:\\nMedial joint pain\\n\\n### Input2:\\nMENISCI: Marked destruction of the posterior horn of the medial meniscus without extension into the body. Only portions of the medial anchor remains given extensive fragmentation. Anterior horn is otherwise intact. Lateral meniscus is significant for moderate fraying of the inner edge. A small parameniscal cyst is observed adjacent to the anterior horn of the lateral meniscus.ARTICULAR CARTILAGE AND BONE: Mild thinning with focal 1.5 cm defect involving the posterior aspect of the femoral condyle, and overlying the medial meniscal abnormality described above. Mild overlying bone edema. Otherwise moderate changes most pronounced underlying the patella, without additional focal defects. Marrow signal otherwise intact other than scattered degenerative changes with numerous cysts, otherwise observed in the tibial plateau.LIGAMENTS: No significant abnormality noted. EXTENSOR MECHANISM: No significant abnormality noted.ADDITIONAL\\n\\n### Response:\\nLarge knee effusion with associated fragmentation and destruction of the posterior horn of the medial meniscus. See description and the associated overlying adjacent cartilaginous defect.<|end_of_text|>'}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEslMDgOQrcd",
        "outputId": "342e5aba-4183-4cdf-a204-7b53dccf8297"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Entries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 2740/2749 [2:53:24<00:35,  3.91s/entry]"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Assuming you have already loaded your model and tokenizer\n",
        "#tokenizer = tokenizersaved\n",
        "#model = modelsaved\n",
        "\n",
        "\n",
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "\"\"\"\n",
        "# Sample test_dataset for demonstration\n",
        "test_dataset = [\n",
        "    {\n",
        "        'findings': 'MENISCI: Marked destruction of the posterior horn of the medial meniscus without extension into the body.',\n",
        "        'clinical_information': 'Medial joint pain',\n",
        "        'impression': 'Large knee effusion with associated fragmentation and destruction of the posterior horn of the medial meniscus.',\n",
        "        '__index_level_0__': 28822,\n",
        "        'text': '\\n\\n### Input:\\nMedial joint pain\\n\\n### Input2:\\nMENISCI: Marked destruction of the posterior horn of the medial meniscus without extension into the body. \\n\\n### Response:\\nLarge knee effusion with associated fragmentation and destruction of the posterior horn of the medial meniscus.<|end_of_text|>'\n",
        "    }\n",
        "    # Add more entries as needed\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "# Prepare the prompt template\n",
        "alpaca_prompt = \"\\n\\n### Input:\\n{}\\n\\n### Input2:\\n{}\\n\\n### Response:\\n\"\n",
        "\n",
        "# List to store the results\n",
        "results = []\n",
        "\n",
        "# Iterate over the test_dataset with tqdm for progress bar\n",
        "for entry in tqdm(test_dataset, desc=\"Processing Entries\", unit=\"entry\"):\n",
        "    clinical_info = entry['clinical_information']\n",
        "    findings = entry['findings']\n",
        "    actual_impression = entry['impression']\n",
        "\n",
        "    # Prepare the inputs for the model\n",
        "    inputs = tokenizer(\n",
        "        [\n",
        "            alpaca_prompt.format(\n",
        "                clinical_info,\n",
        "                findings,\n",
        "                \"\"  # output - leave this blank for generation!\n",
        "            )\n",
        "        ],\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate the outputs\n",
        "    outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
        "\n",
        "    # Decode the outputs\n",
        "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    # Extract the predicted impression\n",
        "    predicted_impression = generated_texts[0].split(\"### Response:\\n\")[-1].strip()\n",
        "\n",
        "    # Store the results\n",
        "    results.append({\n",
        "        'clinical_information': clinical_info,\n",
        "        'findings': findings,\n",
        "        'predicted_impression': predicted_impression,\n",
        "        'actual_impression': actual_impression\n",
        "    })\n",
        "\n",
        "# Save the results to a JSON file\n",
        "with open('results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "print(\"Results saved to results.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48MKPqFMWGWd",
        "outputId": "94736184-95c1-4da8-8679-5dd2405ac88f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Generating predictions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [19:04<00:00, 32.70s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample Prediction:\n",
            "Clinical Information: Clinical question: Prior MRI with questionable demyelinating disease. Please assess for new lesions. Signs and symptoms: Cognitive decline/depression.\n",
            "Findings: Pre-and post-enhanced MRI:No diffusion weighted abnormalities. Examination redemonstrates stable few subcortical and periventricular foci of flair hyperintensity in bilateral cerebral hemispheres. There is no evidence of any new lesions or any detectable abnormal enhancement.Previously noted lesions within bilateral middle cerebellar peduncles are significantly less conspicuous on current the study and there is no evidence of any new or enhancing lesions in the posterior fossa.Unremarkable cerebral cortex, cortical sulci, ventricular system and the CSF spaces for patient's stated age. No detectable abnormal parenchymal or leptomeningeal enhancement.Unremarkable images through the orbits and including axial fat sat post enhanced series.Unremarkable calvarium, soft tissues of the scalp, paranasal sinuses and mastoid air cells.\n",
            "Predicted Impression: !!!\"!!#!!$!!%!!&!!'!!(!!)!!*!!+!!,!!-!!.!!/!!0!!1!!2!!3!!4!!5!!6!!7!!8!!9!!:!!;!!<!!=!!>!!?!!@!!A!!B!!C!!D!!E!!F!!G!!H!!I!!J!!K!!L!!M!!N!!O!!P!!Q!!R!!\n",
            "Actual Impression: 1.No evidence of an acute or new lesions or any detectable abnormal enhancement.2.Revisualization of stable few subcortical and periventricular flair hyperintensity without enhancement since prior exam.3.Previously reported flair hyperintensity in bilateral middle cerebellar peduncles are significantly less conspicuous and demonstrate no enhancement.\n",
            "\n",
            "Predictions saved to 'mri_Llama8b_predictionsFullDATA_2epoc_stepsclinc.json'\n",
            "CPU times: user 18min 11s, sys: 58 s, total: 19min 9s\n",
            "Wall time: 19min 5s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import gc\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Set environment variable to avoid fragmentation\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Define the Alpaca-style prompt template\n",
        "alpaca_prompt = \"\"\"\n",
        "### Input:\n",
        "{}\n",
        "### Input2:\n",
        "{}\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Function to extract the response part from the generated impression\n",
        "def extract_response(generated_text):\n",
        "    response_start = generated_text.find(\"### Response:\")\n",
        "    if response_start != -1:\n",
        "        return generated_text[response_start + len(\"### Response:\"):].strip()\n",
        "    return generated_text.strip()\n",
        "\n",
        "# max_new_tokens: Maximum number of new tokens to generate\n",
        "def generate_predictions(model, test_dataset, batch_size: int = 12, max_new_tokens: int = 100):\n",
        "    \"\"\"\n",
        "    Generate predictions efficiently using batching and GPU acceleration\n",
        "    \"\"\"\n",
        "    # Setup device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Set the pad token if it's not already set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Move the model to the selected device (GPU or CPU)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Create a DataLoader for the test_dataset to load data in batches\n",
        "    dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize a list to store each prediction as a dictionary\n",
        "    all_predictions = []\n",
        "\n",
        "    print(\"\\nGenerating predictions...\")\n",
        "\n",
        "    try:\n",
        "        # Iterate through each batch in the DataLoader and display progress with tqdm\n",
        "        for batch in tqdm(dataloader):\n",
        "            # Extract findings, clinical information, and actual impressions\n",
        "            batch_findings = batch['findings']\n",
        "            batch_clinical_information = batch['clinical_information']\n",
        "            batch_actual_impressions = batch['impression']\n",
        "\n",
        "            # Tokenize inputs\n",
        "            inputs = tokenizer(\n",
        "                [alpaca_prompt.format(clinical_info, finding, \"\") for clinical_info, finding in zip(batch_clinical_information, batch_findings)],\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=500,  # Adjust based on the findings' length\n",
        "                return_tensors=\"pt\",\n",
        "                return_attention_mask=True  # Explicitly request attention mask\n",
        "            )\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Generate predictions using greedy search\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    input_ids=inputs['input_ids'],\n",
        "                    attention_mask=inputs['attention_mask'],  # Pass attention mask\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    num_return_sequences=1,\n",
        "                    pad_token_id=tokenizer.pad_token_id,\n",
        "                    do_sample=False,  # Use greedy search\n",
        "                    temperature=None,\n",
        "                    num_beams=1,  # Greedy search is equivalent to beam search with num_beams=1\n",
        "                    early_stopping=True,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    length_penalty=1.0\n",
        "                )\n",
        "\n",
        "            # Decode the generated token sequences back into readable text\n",
        "            generated_impressions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "            # Extract the response part from the generated impressions\n",
        "            generated_impressions = [extract_response(imp) for imp in generated_impressions]\n",
        "\n",
        "            # Append each prediction as a dictionary to the list\n",
        "            for clinical_info, finding, pred_imp, actual_imp in zip(batch_clinical_information, batch_findings, generated_impressions, batch_actual_impressions):\n",
        "                all_predictions.append({\n",
        "                    'clinical_information': clinical_info,\n",
        "                    'findings': finding,\n",
        "                    'predicted_impression': pred_imp,\n",
        "                    'actual_impression': actual_imp\n",
        "                })\n",
        "\n",
        "            # Clear GPU cache with torch.cuda.empty_cache() and use garbage collection (gc.collect()) to manage memory efficiently\n",
        "            del inputs, outputs, batch_clinical_information, batch_findings, batch_actual_impressions\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during processing: {str(e)}\")\n",
        "        raise e\n",
        "\n",
        "    finally:\n",
        "        # Clean up GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Generate predictions\n",
        "try:\n",
        "    predictions = generate_predictions(\n",
        "        model=model,\n",
        "        test_dataset=test_dataset,\n",
        "        batch_size=80,\n",
        "        max_new_tokens=150\n",
        "    )\n",
        "\n",
        "    # Print a sample prediction\n",
        "    print(\"\\nSample Prediction:\")\n",
        "    sample_prediction = predictions[0]  # Assuming there is at least one prediction\n",
        "    print(\"Clinical Information:\", sample_prediction['clinical_information'])\n",
        "    print(\"Findings:\", sample_prediction['findings'])\n",
        "    print(\"Predicted Impression:\", sample_prediction['predicted_impression'])\n",
        "    print(\"Actual Impression:\", sample_prediction['actual_impression'])\n",
        "\n",
        "    # Save predictions to file\n",
        "    with open('mri_Llama8b_predictionsFullDATA_2epoc_stepsclinc.json', 'w') as f:\n",
        "        json.dump(predictions, f, indent=2)\n",
        "\n",
        "    print(\"\\nPredictions saved to 'mri_Llama8b_predictionsFullDATA_2epoc_stepsclinc.json'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05646d6858934bf09f6415ed80c5f2b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0821fc7f6fec4fbe816c82841121edee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3d2284a05f7424887c3c11387258697",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f56fb0adbaeb4465b9879ae615b8037e",
            "value": "‚Äá2748/2748‚Äá[00:03&lt;00:00,‚Äá1163.03‚Äáexamples/s]"
          }
        },
        "125a412329ae4e0c8010a664f588b7aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1363f9bef4b24db4b21ba5aa43ad55fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20e585fb1a97455f99ce317998c0fa51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2df001d63e544a1e9a2b39471c2b64c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3062dbbfa2a04c0cb63acdde2646b799": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3364c56ad01e48478633cdc44370c43d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3709d36325f4441d88171c27487ecf34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b39fd62f15143669bf5783a2e9b7c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40be3012f4da44af820caf46bc154480": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d8a326bd297498db74c5f5e88c1acbe",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3364c56ad01e48478633cdc44370c43d",
            "value": "Map‚Äá(num_proc=2):‚Äá100%"
          }
        },
        "58a2e0cc3a8944dfaee70e8bf662c17e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2df001d63e544a1e9a2b39471c2b64c2",
            "max": 27484,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20e585fb1a97455f99ce317998c0fa51",
            "value": 27484
          }
        },
        "6286526939004191a918458e3996a853": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "652d4ee791964e928d28cb9c7f4c5684": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea3db4204db945c1b6886e2d95315061",
              "IPY_MODEL_89b006bdca0f40e89ff986ac81d943c5",
              "IPY_MODEL_a647a2c063f34829a8219bf3d73594c7"
            ],
            "layout": "IPY_MODEL_125a412329ae4e0c8010a664f588b7aa"
          }
        },
        "7e5b4cb550004fd9abf2c5b0ebd72857": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89b006bdca0f40e89ff986ac81d943c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1363f9bef4b24db4b21ba5aa43ad55fc",
            "max": 21987,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a59ee7c1299c40f1afcde5e3fefa705d",
            "value": 21987
          }
        },
        "8b3ed67717694c5987cbd87bee4b7825": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6286526939004191a918458e3996a853",
            "max": 2748,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8be3584ab8c4d6e8b512e115eff7880",
            "value": 2748
          }
        },
        "8d8a326bd297498db74c5f5e88c1acbe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9314d10eafd54b7db4ed1d1026b17a6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c87a2f2bfd243aba656095f4358351f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2b78e931f064940a721c1226e7fb47e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40be3012f4da44af820caf46bc154480",
              "IPY_MODEL_8b3ed67717694c5987cbd87bee4b7825",
              "IPY_MODEL_0821fc7f6fec4fbe816c82841121edee"
            ],
            "layout": "IPY_MODEL_cc8fd36d8f2d41aa9388fc374041e08e"
          }
        },
        "a59ee7c1299c40f1afcde5e3fefa705d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a647a2c063f34829a8219bf3d73594c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3709d36325f4441d88171c27487ecf34",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7e5b4cb550004fd9abf2c5b0ebd72857",
            "value": "‚Äá21987/21987‚Äá[00:13&lt;00:00,‚Äá1954.84‚Äáexamples/s]"
          }
        },
        "b8f403f704534f1a8b71de001f2ac038": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9422bd2ad8f47e987ad247f34a3686d",
              "IPY_MODEL_58a2e0cc3a8944dfaee70e8bf662c17e",
              "IPY_MODEL_e4968b65b5904f5ba95d662bb8ca171a"
            ],
            "layout": "IPY_MODEL_9314d10eafd54b7db4ed1d1026b17a6d"
          }
        },
        "c9422bd2ad8f47e987ad247f34a3686d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc3d41938fed42ed98ef94f90220fa5b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9c87a2f2bfd243aba656095f4358351f",
            "value": "Map:‚Äá100%"
          }
        },
        "cc3d41938fed42ed98ef94f90220fa5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc8fd36d8f2d41aa9388fc374041e08e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1d9552e1dd7438ea1f8c66f669c97dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8be3584ab8c4d6e8b512e115eff7880": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3d2284a05f7424887c3c11387258697": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4968b65b5904f5ba95d662bb8ca171a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05646d6858934bf09f6415ed80c5f2b9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3062dbbfa2a04c0cb63acdde2646b799",
            "value": "‚Äá27484/27484‚Äá[00:00&lt;00:00,‚Äá65922.45‚Äáexamples/s]"
          }
        },
        "ea3db4204db945c1b6886e2d95315061": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b39fd62f15143669bf5783a2e9b7c4e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d1d9552e1dd7438ea1f8c66f669c97dc",
            "value": "Map‚Äá(num_proc=2):‚Äá100%"
          }
        },
        "f56fb0adbaeb4465b9879ae615b8037e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}