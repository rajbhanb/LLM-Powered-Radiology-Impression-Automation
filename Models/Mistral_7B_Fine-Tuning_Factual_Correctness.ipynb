{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqM-T1RTzY6C"
   },
   "source": [
    "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
    "<div class=\"align-center\">\n",
    "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
    "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
    "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
    "</div>\n",
    "\n",
    "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions).\n",
    "\n",
    "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).\n",
    "\n",
    "[NEW] Llama-3.1 8b, 70b & 405b are trained on a crazy 15 trillion tokens with 128K long context lengths!\n",
    "\n",
    "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eSvM9zX_2d3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth \"xformers==0.0.28.post2\"\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOCp9ilKofzF"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U bitsandbytes transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2v_X2fA0Df5"
   },
   "source": [
    "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
    "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
    "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
    "* [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
    "* [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
    "* [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Mistral 7B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "9bc6f2b2-91d7-47e9-ac83-4409f1503e81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.10: Fast Mistral patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "CPU times: user 10.3 s, sys: 1.91 s, total: 12.2 s\n",
      "Wall time: 28.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 500 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-v0.3\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Stw-9tA61CU",
    "outputId": "a4a2712f-b9bf-4f60-95b1-71dd2c710733"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096, padding_idx=770)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "91fef514-2f03-4407-c380-61fe1119259b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.10 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
    "\n",
    "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
    "\n",
    "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
    "\n",
    "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
    "\n",
    "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOH1TE_iK2DP",
    "outputId": "4084f19c-622b-4c40-a227-efb5916fdd71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Umm6YYK5KPJ3",
    "outputId": "3f89880a-a21a-4b86-ae4d-71e0fb6cdfab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 22 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   findings                 100 non-null    object \n",
      " 1   clinical_information     100 non-null    object \n",
      " 2   actual_impression        100 non-null    object \n",
      " 3   predicted_impression     100 non-null    object \n",
      " 4   Factual_Correctness      100 non-null    float64\n",
      " 5   ClinicalBERT Similarity  100 non-null    float64\n",
      " 6   RadBERT Similarity       100 non-null    float64\n",
      " 7   Rogue L Similarity       100 non-null    float64\n",
      " 8   BERTScore Precision      100 non-null    float64\n",
      " 9   BERTScore Recall         100 non-null    float64\n",
      " 10  BERTScore F1             100 non-null    float64\n",
      " 11  Rouge1 Precision         100 non-null    float64\n",
      " 12  Rouge1 Recall            100 non-null    float64\n",
      " 13  Rouge1 F1                100 non-null    float64\n",
      " 14  Rouge2 Precision         100 non-null    float64\n",
      " 15  Rouge2 Recall            100 non-null    float64\n",
      " 16  Rouge2 F1                100 non-null    float64\n",
      " 17  RougeL Precision         100 non-null    float64\n",
      " 18  RougeL Recall            100 non-null    float64\n",
      " 19  RougeL F1                100 non-null    float64\n",
      " 20  Bleu Score               100 non-null    float64\n",
      " 21  Row number               100 non-null    float64\n",
      "dtypes: float64(18), object(4)\n",
      "memory usage: 17.3+ KB\n",
      "CPU times: user 20.4 ms, sys: 3.5 ms, total: 23.8 ms\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset\n",
    "MRIdf = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Capstone2/FinalResults/sample_results_with_details_FactualCorrectness.csv')\n",
    "\n",
    "# Remove rows with missing values in the relevant columns\n",
    "\n",
    "MRIdf = MRIdf.dropna(subset=['actual_impression', 'predicted_impression', 'Factual_Correctness'])\n",
    "MRIdf.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal is to fine-tune model to generate factual correctness "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**100 samples of orginal impression and generated impression were manually given factual correctness scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622,
     "referenced_widgets": [
      "8873d08879d74d389894eb5a93a71799",
      "dcc82b220d77448a9570cb99170370f5",
      "2c004e5d9f54467a88a03bfc63ca30d2",
      "0d5b565efed54b289eed27b2905ba58a",
      "cb7fffe1277c45f29e6315388cf49672",
      "4e8ebd8a1d32403c91ba7359689acd6e",
      "3bc446ab29354c01a905e2be0928c908",
      "2efb5d01b157409b8b398770ba49d506",
      "2bd94c511f2e4674b0dbcd5195277be8",
      "bd281e0cf7344978a74d8ebafd5f055a",
      "da13c8b4a18b445db437ebbd9008214a"
     ]
    },
    "id": "phJxNC5ZU6VL",
    "outputId": "0f7a5076-5e0b-4cb0-f858-89c94b1bfb15"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8873d08879d74d389894eb5a93a71799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60.6 ms, sys: 583 µs, total: 61.2 ms\n",
      "Wall time: 200 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'actual_impression': ['1.No evidence of an acute or new lesions or any detectable abnormal enhancement.2.Revisualization of stable few subcortical and periventricular flair hyperintensity without enhancement since prior exam.3.Previously reported flair hyperintensity in bilateral middle cerebellar peduncles are significantly less conspicuous and demonstrate no enhancement.',\n",
       "  'Large knee effusion with associated fragmentation and destruction of the posterior horn of the medial meniscus. See description and the associated overlying adjacent cartilaginous defect.'],\n",
       " 'predicted_impression': ['1.Stable few subcortical and periventricular foci of flair hyperintensity in bilateral cerebral hemispheres.2.No evidence of any new lesions or any detectable abnormal enhancement.3.Previously noted lesions within bilateral middle cerebellar peduncles are significantly less conspicuous on current the study and there is no evidence of any new or enhancing lesions in the posterior fossa.',\n",
       "  '1. Marked destruction of the posterior horn of the medial meniscus without extension into the body. Only portions of the medial anchor remains given extensive fragmentation. Anterior horn is otherwise intact.2. Mild thinning with focal 1.5 cm defect involving the posterior aspect of the femoral condyle, and overlying the medial meniscal abnormality described above. Mild overlying bone edema. Otherwise moderate changes most pronounced underlying the patella'],\n",
       " 'Factual_Correctness': ['6.0', '4.0'],\n",
       " 'text': ['\\n\\n### Input:\\n1.No evidence of an acute or new lesions or any detectable abnormal enhancement.2.Revisualization of stable few subcortical and periventricular flair hyperintensity without enhancement since prior exam.3.Previously reported flair hyperintensity in bilateral middle cerebellar peduncles are significantly less conspicuous and demonstrate no enhancement.\\n\\n### Input2:\\n1.Stable few subcortical and periventricular foci of flair hyperintensity in bilateral cerebral hemispheres.2.No evidence of any new lesions or any detectable abnormal enhancement.3.Previously noted lesions within bilateral middle cerebellar peduncles are significantly less conspicuous on current the study and there is no evidence of any new or enhancing lesions in the posterior fossa.\\n\\n### Response:\\n6.0</s>',\n",
       "  '\\n\\n### Input:\\nLarge knee effusion with associated fragmentation and destruction of the posterior horn of the medial meniscus. See description and the associated overlying adjacent cartilaginous defect.\\n\\n### Input2:\\n1. Marked destruction of the posterior horn of the medial meniscus without extension into the body. Only portions of the medial anchor remains given extensive fragmentation. Anterior horn is otherwise intact.2. Mild thinning with focal 1.5 cm defect involving the posterior aspect of the femoral condyle, and overlying the medial meniscal abnormality described above. Mild overlying bone edema. Otherwise moderate changes most pronounced underlying the patella\\n\\n### Response:\\n4.0</s>']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "# Load only the required columns and drop any rows with missing values in those columns\n",
    "MRIdf_subset = MRIdf[['actual_impression', 'predicted_impression', 'Factual_Correctness']].dropna().drop_duplicates()\n",
    "\n",
    "# Ensure that each column is of type string\n",
    "MRIdf_subset['actual_impression'] = MRIdf_subset['actual_impression'].astype(str)\n",
    "MRIdf_subset['predicted_impression'] = MRIdf_subset['predicted_impression'].astype(str)\n",
    "MRIdf_subset['Factual_Correctness'] = MRIdf_subset['Factual_Correctness'].astype(str)\n",
    "\n",
    "# Convert the subset DataFrame to a Hugging Face Dataset\n",
    "MRIdf_dataset = Dataset.from_pandas(MRIdf_subset)\n",
    "\n",
    "# Define the Alpaca-style prompt template\n",
    "alpaca_prompt = \"\"\"\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Input2:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# EOS token for end-of-sequence in generated text\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# Formatting function to structure input-output pairs\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"actual_impression\"]\n",
    "    inputs2 = examples[\"predicted_impression\"]\n",
    "    outputs = examples[\"Factual_Correctness\"]\n",
    "    texts = []\n",
    "    for input, input2, output in zip(inputs, inputs2, outputs):\n",
    "        # Use Alpaca prompt template, adding EOS token at the end\n",
    "        text = alpaca_prompt.format(input, input2, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting function to dataset with batched processing\n",
    "formatted_data = MRIdf_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Show the first few formatted examples\n",
    "formatted_data[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nAmp4ztDjbkA",
    "outputId": "b9b4dbfa-4b40-429a-91b8-a33cb4ec4181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 21987\n",
      "Validation samples: 2748\n",
      "Test samples: 2749\n",
      "CPU times: user 310 ms, sys: 269 ms, total: 579 ms\n",
      "Wall time: 605 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert the formatted dataset to a DataFrame for train-test split, and reset index to avoid duplicate index errors\n",
    "formatted_df = formatted_data.to_pandas().reset_index(drop=True)\n",
    "\n",
    "# First split into train and temp (80-20)\n",
    "train_data, temp_data = train_test_split(formatted_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split temp into validation and test (50-50, meaning 10-10 of original data)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert back to Hugging Face Datasets without additional indices\n",
    "train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\n",
    "val_dataset = Dataset.from_pandas(val_data.reset_index(drop=True))\n",
    "test_dataset = Dataset.from_pandas(test_data.reset_index(drop=True))\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AV5ZMf74Pfug",
    "outputId": "d7e78b36-37c7-440f-d8a2-9a056c233b2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'findings': 'There is a heterogeneously enhancing, low T1 and T2 signal intensity left temporal region calvarium lesion that measures up to 5 cm in length with epidural extension that measures up to 7 mm in thickness. There is mild mass effect upon the underlying brain parenchyma. There also appears to be mild subgaleal extension of tumor. There is no evidence of intracranial hemorrhage or acute infarct. There is mild nonspecific periventricular cerebral white matter T2 hyperintensity, but no evidence of intraparenchymal enhancing lesions. The ventricles and basal cisterns are normal in size and configuration. There is no midline shift or herniation. The major cerebral flow voids are intact. The orbits are grossly unremarkable.',\n",
       " 'clinical_information': 'Metastatic breast cancer.',\n",
       " 'impression': 'Left temporal calvarium region metastasis with epidural extension measuring up to 7 mm in width. ',\n",
       " '__index_level_0__': 16548,\n",
       " 'text': '\\n\\n### Input:\\nMetastatic breast cancer.\\n\\n### Input2:\\nThere is a heterogeneously enhancing, low T1 and T2 signal intensity left temporal region calvarium lesion that measures up to 5 cm in length with epidural extension that measures up to 7 mm in thickness. There is mild mass effect upon the underlying brain parenchyma. There also appears to be mild subgaleal extension of tumor. There is no evidence of intracranial hemorrhage or acute infarct. There is mild nonspecific periventricular cerebral white matter T2 hyperintensity, but no evidence of intraparenchymal enhancing lesions. The ventricles and basal cisterns are normal in size and configuration. There is no midline shift or herniation. The major cerebral flow voids are intact. The orbits are grossly unremarkable.\\n\\n### Response:\\nLeft temporal calvarium region metastasis with epidural extension measuring up to 7 mm in width. </s>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "2e5503640864458ebee6ff2692e517d1",
      "fb4b43f125444536911db89a82981af8",
      "8e7d006a00b84216ba04fda497697160",
      "82f946e9d5b34859b6501ece3c411dd9",
      "e85e2adebf114c51923d1d614745792d",
      "6fd1fd381e43465c9af8570e310b6295",
      "cc906adf7337401d8c3bd9cf66e5fd96",
      "7c6a4c4690a54474ac326396c6834549",
      "76765cba2c6347a7a6349adf0d2fd515",
      "a2e2db6c4b0544cc826ffc14c88805bd",
      "1a459f210e2248638b4b8fc9d38c150e"
     ]
    },
    "id": "95_Nn-89DhsL",
    "outputId": "f5d3d805-96cf-4c2d-e3af-e1903c46362d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5503640864458ebee6ff2692e517d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "     train_dataset=formatted_data,\n",
    "    #eval_dataset=val_dataset,  # Added validation dataset\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "       # num_train_epochs = 10, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "3dca754a-003f-4f92-a1b1-6ec8b811ebce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = Tesla T4. Max memory = 14.748 GB.\n",
      "11.582 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "2b003752-1235-49f3-99e5-9bf4b0035738"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 64 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 45:29, Epoch 34/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.779400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.355700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.749000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.215300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.556600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.124900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.393700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.118800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.966800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.727900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.701100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.385400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.447500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.295600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.344300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.241900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.102300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.080600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.085100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.086500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.078700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.063600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.057900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.062300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.064600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.038600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.042500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26min 23s, sys: 16min 36s, total: 42min 59s\n",
      "Wall time: 46min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqOV7wdzkYLh",
    "outputId": "927fa2f7-6a69-4c7c-a466-7c5215dd46c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 256 ms, sys: 137 ms, total: 393 ms\n",
      "Wall time: 6.03 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsMRIMistralb60stepsclinc/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsMRIMistralb60stepsclinc/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsMRIMistralb60stepsclinc/tokenizer.model',\n",
       " '/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsMRIMistralb60stepsclinc/added_tokens.json',\n",
       " '/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsMRIMistralb60stepsclinc/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Save the trained model and tokenizer\n",
    "model.save_pretrained('/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsmisrral7bFC')\n",
    "tokenizer.save_pretrained('/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsmisrral7bFC')\n",
    "\n",
    "#Save training arguments\n",
    "#torch.save(training_params, '/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsMRILlama8b60steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bd2YvrZwmjmj",
    "outputId": "c76ebf6f-4841-4768-997c-dc070ab22fd7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 s, sys: 5.96 s, total: 23.7 s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "#Reload Model\n",
    "%%time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "modelsaved = AutoModelForCausalLM.from_pretrained('/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsmisrral7bFC')\n",
    "tokenizersaved = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Colab Notebooks/Capstone2/classicsmisrral7bFC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!\n",
    "\n",
    "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SNno2S-hMld",
    "outputId": "1f093333-ae3a-4185-ef36-01da40cf1b0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "\n",
      "### Input:\n",
      "1.No evidence of an acute or new lesions or any detectable abnormal enhancement.2.Revisualization of stable few subcortical and periventricular flair hyperintensity without enhancement since prior exam.3.Previously reported flair hyperintensity in bilateral middle cerebellar peduncles are significantly less conspicuous and demonstrate no enhancement.\n",
      "\n",
      "### Input2:\n",
      "1.Stable few subcortical and periventricular foci of flair hyperintensity in bilateral cerebral hemispheres.2.No evidence of any new lesions or any detectable abnormal enhancement.3.Previously noted lesions within bilateral middle cerebellar peduncles are significantly less conspicuous on current the study and there is no evidence of any new or enhancing lesions in the posterior fossa.\n",
      "\n",
      "### Response:\n",
      "5.0</s>\n"
     ]
    }
   ],
   "source": [
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Prepare the inputs for the model\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            \"1.No evidence of an acute or new lesions or any detectable abnormal enhancement.2.Revisualization of stable few subcortical and periventricular flair hyperintensity without enhancement since prior exam.3.Previously reported flair hyperintensity in bilateral middle cerebellar peduncles are significantly less conspicuous and demonstrate no enhancement.\",\n",
    "            \"1.Stable few subcortical and periventricular foci of flair hyperintensity in bilateral cerebral hemispheres.2.No evidence of any new lesions or any detectable abnormal enhancement.3.Previously noted lesions within bilateral middle cerebellar peduncles are significantly less conspicuous on current the study and there is no evidence of any new or enhancing lesions in the posterior fossa.\",  # input\n",
    "            \"\"  # output - leave this blank for generation!\n",
    "        )\n",
    "    ],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate the outputs\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "\n",
    "# Decode the outputs\n",
    "generated_texts = tokenizer.batch_decode(outputs)\n",
    "\n",
    "# Print the generated text\n",
    "for text in generated_texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Factual Correctness for top 5 model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0AgRJka9dzSL",
    "outputId": "65248871-4198-4fa9-f819-f6415c9e262d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries: 100%|██████████| 2749/2749 [29:06<00:00,  1.57entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to slothMistral_22b_factual_correctness.json\n",
      "CPU times: user 26min 27s, sys: 1min 30s, total: 27min 57s\n",
      "Wall time: 29min 8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model and tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained('your-model-path')\n",
    "#model = AutoModelForCausalLM.from_pretrained('your-model-path')\n",
    "\n",
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Prepare the prompt template\n",
    "alpaca_prompt = \"\\n\\n### Input:\\n{}\\n\\n### Input2:\\n{}\\n\\n### Response:\\n\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/Capstone2/FinalResults/results_mistral 22b.json', 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over the test_dataset with tqdm for progress bar\n",
    "for entry in tqdm(test_dataset, desc=\"Processing Entries\", unit=\"entry\"):\n",
    "    actual_impression = entry['actual_impression']\n",
    "    predicted_impression = entry['predicted_impression']\n",
    "\n",
    "\n",
    "    # Prepare the inputs for the model\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            alpaca_prompt.format(\n",
    "                actual_impression,\n",
    "                predicted_impression,\n",
    "                \"\"  # output - leave this blank for generation!\n",
    "            )\n",
    "        ],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate the outputs\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
    "\n",
    "    # Decode the outputs\n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # Extract the factual correction\n",
    "    factual_correction = generated_texts[0].split(\"### Response:\\n\")[-1].strip()\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'clinical_information': entry['clinical_information'],\n",
    "        'findings': entry['findings'],\n",
    "        'predicted_impression': entry['predicted_impression'],\n",
    "        'actual_impression': entry['actual_impression'],\n",
    "        'factual_correction': factual_correction\n",
    "    })\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('slothMistral_22b_factual_correctness.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Results saved to slothMistral_22b_factual_correctness.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgjnypFvw5fW",
    "outputId": "aca74680-6e7b-475c-dfef-b3636ef99b7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries: 100%|██████████| 2749/2749 [29:06<00:00,  1.57entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to slothMistral_7b_factual_correctness.json\n",
      "CPU times: user 26min 5s, sys: 1min 28s, total: 27min 34s\n",
      "Wall time: 29min 8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model and tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained('your-model-path')\n",
    "#model = AutoModelForCausalLM.from_pretrained('your-model-path')\n",
    "\n",
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Prepare the prompt template\n",
    "alpaca_prompt = \"\\n\\n### Input:\\n{}\\n\\n### Input2:\\n{}\\n\\n### Response:\\n\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/Capstone2/FinalResults/slothMistral_7b_2ndattempt.json', 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over the test_dataset with tqdm for progress bar\n",
    "for entry in tqdm(test_dataset, desc=\"Processing Entries\", unit=\"entry\"):\n",
    "    actual_impression = entry['actual_impression']\n",
    "    predicted_impression = entry['predicted_impression']\n",
    "\n",
    "\n",
    "    # Prepare the inputs for the model\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            alpaca_prompt.format(\n",
    "                actual_impression,\n",
    "                predicted_impression,\n",
    "                \"\"  # output - leave this blank for generation!\n",
    "            )\n",
    "        ],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate the outputs\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
    "\n",
    "    # Decode the outputs\n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # Extract the factual correction\n",
    "    factual_correction = generated_texts[0].split(\"### Response:\\n\")[-1].strip()\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'clinical_information': entry['clinical_information'],\n",
    "        'findings': entry['findings'],\n",
    "        'predicted_impression': entry['predicted_impression'],\n",
    "        'actual_impression': entry['actual_impression'],\n",
    "        'factual_correction': factual_correction\n",
    "    })\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('slothMistral_7b_factual_correctness.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Results saved to slothMistral_7b_factual_correctness.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PrTZh8ZryFWe",
    "outputId": "cba5138d-f2da-4cad-8efa-1bdbfdfac93e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries: 100%|██████████| 2749/2749 [28:27<00:00,  1.61entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to slothLlamal_8b_factual_correctness.json\n",
      "CPU times: user 26min 3s, sys: 1min 37s, total: 27min 41s\n",
      "Wall time: 28min 27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model and tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained('your-model-path')\n",
    "#model = AutoModelForCausalLM.from_pretrained('your-model-path')\n",
    "\n",
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Prepare the prompt template\n",
    "alpaca_prompt = \"\\n\\n### Input:\\n{}\\n\\n### Input2:\\n{}\\n\\n### Response:\\n\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/Capstone2/FinalResults/llama8b_2ndattempt_results.json', 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over the test_dataset with tqdm for progress bar\n",
    "for entry in tqdm(test_dataset, desc=\"Processing Entries\", unit=\"entry\"):\n",
    "    actual_impression = entry['actual_impression']\n",
    "    predicted_impression = entry['predicted_impression']\n",
    "\n",
    "\n",
    "    # Prepare the inputs for the model\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            alpaca_prompt.format(\n",
    "                actual_impression,\n",
    "                predicted_impression,\n",
    "                \"\"  # output - leave this blank for generation!\n",
    "            )\n",
    "        ],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate the outputs\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
    "\n",
    "    # Decode the outputs\n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # Extract the factual correction\n",
    "    factual_correction = generated_texts[0].split(\"### Response:\\n\")[-1].strip()\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'clinical_information': entry['clinical_information'],\n",
    "        'findings': entry['findings'],\n",
    "        'predicted_impression': entry['predicted_impression'],\n",
    "        'actual_impression': entry['actual_impression'],\n",
    "        'factual_correction': factual_correction\n",
    "    })\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('slothLlamal_8b_factual_correctness.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Results saved to slothLlamal_8b_factual_correctness.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uTPGfo3yNnt",
    "outputId": "411f7dae-9cfa-453d-c52d-718e6d4b8c18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries: 100%|██████████| 2749/2749 [28:18<00:00,  1.62entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to slothLlamal_1b_factual_correctness.json\n",
      "CPU times: user 26min 9s, sys: 1min 33s, total: 27min 43s\n",
      "Wall time: 28min 19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model and tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained('your-model-path')\n",
    "#model = AutoModelForCausalLM.from_pretrained('your-model-path')\n",
    "\n",
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Prepare the prompt template\n",
    "alpaca_prompt = \"\\n\\n### Input:\\n{}\\n\\n### Input2:\\n{}\\n\\n### Response:\\n\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/Capstone2/FinalResults/slothllama_1b_2ndattempt.json', 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over the test_dataset with tqdm for progress bar\n",
    "for entry in tqdm(test_dataset, desc=\"Processing Entries\", unit=\"entry\"):\n",
    "    actual_impression = entry['actual_impression']\n",
    "    predicted_impression = entry['predicted_impression']\n",
    "\n",
    "\n",
    "    # Prepare the inputs for the model\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            alpaca_prompt.format(\n",
    "                actual_impression,\n",
    "                predicted_impression,\n",
    "                \"\"  # output - leave this blank for generation!\n",
    "            )\n",
    "        ],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate the outputs\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
    "\n",
    "    # Decode the outputs\n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # Extract the factual correction\n",
    "    factual_correction = generated_texts[0].split(\"### Response:\\n\")[-1].strip()\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'clinical_information': entry['clinical_information'],\n",
    "        'findings': entry['findings'],\n",
    "        'predicted_impression': entry['predicted_impression'],\n",
    "        'actual_impression': entry['actual_impression'],\n",
    "        'factual_correction': factual_correction\n",
    "    })\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('slothLlamal_1b_factual_correctness.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Results saved to slothLlamal_1b_factual_correctness.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kf9chAtTyle3",
    "outputId": "79e16523-e19d-45a1-e8fc-43e427c1e41b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries: 100%|██████████| 2749/2749 [28:33<00:00,  1.60entry/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to slothLlamal_3b_factual_correctness.json\n",
      "CPU times: user 26min 16s, sys: 1min 33s, total: 27min 50s\n",
      "Wall time: 28min 34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model and tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained('your-model-path')\n",
    "#model = AutoModelForCausalLM.from_pretrained('your-model-path')\n",
    "\n",
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Prepare the prompt template\n",
    "alpaca_prompt = \"\\n\\n### Input:\\n{}\\n\\n### Input2:\\n{}\\n\\n### Response:\\n\"\n",
    "\n",
    "# Load the JSON file\n",
    "with open('/content/drive/MyDrive/Colab Notebooks/Capstone2/FinalResults/slothllama_3b_2ndattempt.json', 'r') as f:\n",
    "    test_dataset = json.load(f)\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over the test_dataset with tqdm for progress bar\n",
    "for entry in tqdm(test_dataset, desc=\"Processing Entries\", unit=\"entry\"):\n",
    "    actual_impression = entry['actual_impression']\n",
    "    predicted_impression = entry['predicted_impression']\n",
    "\n",
    "\n",
    "    # Prepare the inputs for the model\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            alpaca_prompt.format(\n",
    "                actual_impression,\n",
    "                predicted_impression,\n",
    "                \"\"  # output - leave this blank for generation!\n",
    "            )\n",
    "        ],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate the outputs\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
    "\n",
    "    # Decode the outputs\n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # Extract the factual correction\n",
    "    factual_correction = generated_texts[0].split(\"### Response:\\n\")[-1].strip()\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'clinical_information': entry['clinical_information'],\n",
    "        'findings': entry['findings'],\n",
    "        'predicted_impression': entry['predicted_impression'],\n",
    "        'actual_impression': entry['actual_impression'],\n",
    "        'factual_correction': factual_correction\n",
    "    })\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('slothLlamal_3b_factual_correctness.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Results saved to slothLlamal_3b_factual_correctness.json\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d5b565efed54b289eed27b2905ba58a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd281e0cf7344978a74d8ebafd5f055a",
      "placeholder": "​",
      "style": "IPY_MODEL_da13c8b4a18b445db437ebbd9008214a",
      "value": " 100/100 [00:00&lt;00:00, 1057.69 examples/s]"
     }
    },
    "1a459f210e2248638b4b8fc9d38c150e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2bd94c511f2e4674b0dbcd5195277be8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2c004e5d9f54467a88a03bfc63ca30d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2efb5d01b157409b8b398770ba49d506",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2bd94c511f2e4674b0dbcd5195277be8",
      "value": 100
     }
    },
    "2e5503640864458ebee6ff2692e517d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fb4b43f125444536911db89a82981af8",
       "IPY_MODEL_8e7d006a00b84216ba04fda497697160",
       "IPY_MODEL_82f946e9d5b34859b6501ece3c411dd9"
      ],
      "layout": "IPY_MODEL_e85e2adebf114c51923d1d614745792d"
     }
    },
    "2efb5d01b157409b8b398770ba49d506": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bc446ab29354c01a905e2be0928c908": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e8ebd8a1d32403c91ba7359689acd6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fd1fd381e43465c9af8570e310b6295": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76765cba2c6347a7a6349adf0d2fd515": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7c6a4c4690a54474ac326396c6834549": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "82f946e9d5b34859b6501ece3c411dd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2e2db6c4b0544cc826ffc14c88805bd",
      "placeholder": "​",
      "style": "IPY_MODEL_1a459f210e2248638b4b8fc9d38c150e",
      "value": " 100/100 [00:00&lt;00:00, 65.87 examples/s]"
     }
    },
    "8873d08879d74d389894eb5a93a71799": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dcc82b220d77448a9570cb99170370f5",
       "IPY_MODEL_2c004e5d9f54467a88a03bfc63ca30d2",
       "IPY_MODEL_0d5b565efed54b289eed27b2905ba58a"
      ],
      "layout": "IPY_MODEL_cb7fffe1277c45f29e6315388cf49672"
     }
    },
    "8e7d006a00b84216ba04fda497697160": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c6a4c4690a54474ac326396c6834549",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76765cba2c6347a7a6349adf0d2fd515",
      "value": 100
     }
    },
    "a2e2db6c4b0544cc826ffc14c88805bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd281e0cf7344978a74d8ebafd5f055a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb7fffe1277c45f29e6315388cf49672": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc906adf7337401d8c3bd9cf66e5fd96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da13c8b4a18b445db437ebbd9008214a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dcc82b220d77448a9570cb99170370f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e8ebd8a1d32403c91ba7359689acd6e",
      "placeholder": "​",
      "style": "IPY_MODEL_3bc446ab29354c01a905e2be0928c908",
      "value": "Map: 100%"
     }
    },
    "e85e2adebf114c51923d1d614745792d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb4b43f125444536911db89a82981af8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fd1fd381e43465c9af8570e310b6295",
      "placeholder": "​",
      "style": "IPY_MODEL_cc906adf7337401d8c3bd9cf66e5fd96",
      "value": "Map (num_proc=2): 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
